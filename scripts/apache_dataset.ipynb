{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##  Building a commit classification dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import javalang\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "from unidiff import PatchSet\n",
    "\n",
    "\n",
    "# List of repositories that will make up the dataset\n",
    "repositories = ['apache/hadoop', 'apache/hive']\n",
    "\n",
    "label_encoding = {\n",
    "    'Bug': '001', \n",
    "    'Improvement': '010', \n",
    "    'Sub-task': '100'\n",
    "}\n",
    "\n",
    "# Filename extension for code files\n",
    "code_ext = '.java'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "def is_ascii(string): \n",
    "    return all(ord(c) < 128 for c in string)\n",
    "\n",
    "def is_code_change(diff, code_ext):\n",
    "    \"\"\"\n",
    "    Returns true if the diff contains changes from at least one code file\n",
    "    :param diff: pull request diff\n",
    "    :param code_ext: filename extension for code files\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    patch = PatchSet(diff)\n",
    "    for file in patch.added_files + patch.modified_files + patch.removed_files:\n",
    "        if os.path.splitext(file.path)[1] == code_ext:\n",
    "            return True\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def get_tokens(code_changes):\n",
    "    \"\"\"\n",
    "    Returns tokens from Java code snippets.\n",
    "    NOTE: This requires well formed Java code as input and doesn't work on diffs\n",
    "    :param code_changes: Java code encoded as a string\n",
    "    :return: tokens extracted from the input code\n",
    "    \"\"\"\n",
    "    file_tokens = list()\n",
    "\n",
    "    try:\n",
    "        file_tokens = [''.join(x.value.split()) for x in javalang.tokenizer.tokenize('\\n'.join(code_changes))\n",
    "                       if len(x.value.strip()) < 64 and is_ascii(x.value)]\n",
    "            \n",
    "    except:\n",
    "        for line in code_changes:\n",
    "            try:\n",
    "                line_tokens = [''.join(x.value.split()) for x in javalang.tokenizer.tokenize(line)\n",
    "                               if len(x.value.strip()) < 64 and is_ascii(x.value)]\n",
    "                if line_tokens and line_tokens[0] != '*':\n",
    "                    file_tokens.extend(line_tokens)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if file_tokens:\n",
    "        return ' '.join(file_tokens)\n",
    "    \n",
    "    return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param data_x: \n",
    "    :param data_y: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    train_split, test_split = train_test_split(dataset, test_size=0.15, random_state=42)\n",
    "    train_split, dev_split = train_test_split(train_split, test_size=0.176, random_state=42)\n",
    "    print(\"Train, dev and test split sizes:\", len(train_split), len(dev_split), len(test_split))\n",
    "    return train_split, dev_split, test_split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Processed 1000 of 22725 commits in apache/hadoop\n",
      "Processed 2000 of 22725 commits in apache/hadoop\n",
      "Processed 3000 of 22725 commits in apache/hadoop\n",
      "Processed 4000 of 22725 commits in apache/hadoop\n",
      "Processed 5000 of 22725 commits in apache/hadoop\n",
      "Processed 6000 of 22725 commits in apache/hadoop\n",
      "Processed 7000 of 22725 commits in apache/hadoop\n",
      "Processed 8000 of 22725 commits in apache/hadoop\n",
      "Processed 9000 of 22725 commits in apache/hadoop\n",
      "Processed 10000 of 22725 commits in apache/hadoop\n",
      "Processed 11000 of 22725 commits in apache/hadoop\n",
      "Processed 12000 of 22725 commits in apache/hadoop\n",
      "Processed 13000 of 22725 commits in apache/hadoop\n",
      "Processed 14000 of 22725 commits in apache/hadoop\n",
      "Processed 15000 of 22725 commits in apache/hadoop\n",
      "Processed 16000 of 22725 commits in apache/hadoop\n",
      "Processed 17000 of 22725 commits in apache/hadoop\n",
      "Processed 18000 of 22725 commits in apache/hadoop\n",
      "Processed 19000 of 22725 commits in apache/hadoop\n",
      "Processed 20000 of 22725 commits in apache/hadoop\n",
      "Processed 21000 of 22725 commits in apache/hadoop\n",
      "Processed 22000 of 22725 commits in apache/hadoop\n",
      "Processed 1000 of 13676 commits in apache/hive\n",
      "Processed 2000 of 13676 commits in apache/hive\n",
      "Processed 3000 of 13676 commits in apache/hive\n",
      "Processed 4000 of 13676 commits in apache/hive\n",
      "Processed 5000 of 13676 commits in apache/hive\n",
      "Processed 6000 of 13676 commits in apache/hive\n",
      "Processed 7000 of 13676 commits in apache/hive\n",
      "Processed 8000 of 13676 commits in apache/hive\n",
      "Processed 9000 of 13676 commits in apache/hive\n",
      "Processed 10000 of 13676 commits in apache/hive\n",
      "Processed 11000 of 13676 commits in apache/hive\n",
      "Processed 12000 of 13676 commits in apache/hive\n",
      "Processed 13000 of 13676 commits in apache/hive\n",
      "Number of labelled pull requests: 26437\nDataset distribution: defaultdict(<class 'int'>, {'001': 13637, '010': 5310, '100': 7490})\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# List containing labelled pull requests\n",
    "labelled_commits = list()\n",
    "\n",
    "label_counts = defaultdict(int)\n",
    "for repo_name in repositories:\n",
    "    with open(os.path.join(os.pardir, 'data', 'commits', repo_name, 'commit_metadata.json'), 'r') as metadata_file:\n",
    "        metadata_dict = json.load(metadata_file)\n",
    "    with open(os.path.join(os.pardir, 'data', 'commits', repo_name, 'commit_labels.json'), 'r') as label_file:\n",
    "        label_dict = json.load(label_file)\n",
    "    with open(os.path.join(os.pardir, 'data', 'commits', repo_name, 'commit_diffs.json'), 'r') as diff_file:\n",
    "        diff_dict = json.load(diff_file)\n",
    "    \n",
    "    num_processed = 0\n",
    "    for commit in metadata_dict:\n",
    "        diff = diff_dict[commit['sha']]\n",
    "        if is_code_change(diff, code_ext):\n",
    "            commit_label = label_encoding.get(label_dict.get(commit['sha'], None), None)\n",
    "            if commit_label:\n",
    "                labelled_commits.append((commit, repo_name, diff, commit_label))\n",
    "                label_counts[commit_label] += 1\n",
    "        \n",
    "        num_processed += 1\n",
    "        if num_processed % 1000 == 0:\n",
    "            print('Processed %d of %d commits in %s' % \n",
    "                  (num_processed, len(metadata_dict), repo_name))\n",
    "\n",
    "print(\"Number of labelled pull requests:\", len(labelled_commits))\n",
    "print(\"Dataset distribution:\", label_counts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# List containing all the dataset samples\n",
    "dataset = list()\n",
    "\n",
    "for commit, repo_name, diff, label in labelled_commits:\n",
    "    diff_changes = list()\n",
    "    patch = PatchSet(diff)\n",
    "    \n",
    "    for file in patch.added_files + patch.modified_files + patch.removed_files:\n",
    "        if os.path.splitext(file.path)[1] == code_ext:\n",
    "            file_changes = list()\n",
    "            \n",
    "            for hunk in file:\n",
    "                for line in hunk:\n",
    "                    file_changes.append(line.value)\n",
    "            \n",
    "            file_tokens = get_tokens(file_changes)\n",
    "            if file_tokens:\n",
    "                diff_changes.append(file_tokens)\n",
    "    \n",
    "    if diff_changes:\n",
    "        dataset.append((repo_name, \n",
    "                        commit['sha'], \n",
    "                        json.dumps(diff_changes), \n",
    "                        label))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train, dev and test split sizes: 18497 3952 3962\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "train_split, dev_split, test_split = split_dataset(dataset)\n",
    "\n",
    "with open(os.path.join(os.pardir, 'data', 'commits', 'apache', 'train.tsv'), 'w') as tsv_file:\n",
    "    tsv_file.write('\\n'.join('\\t'.join(str(y) for y in x) for x in train_split))\n",
    "with open(os.path.join(os.pardir, 'data', 'commits', 'apache', 'dev.tsv'), 'w') as tsv_file:\n",
    "    tsv_file.write('\\n'.join('\\t'.join(str(y) for y in x) for x in dev_split))\n",
    "with open(os.path.join(os.pardir, 'data', 'commits', 'apache', 'test.tsv'), 'w') as tsv_file:\n",
    "    tsv_file.write('\\n'.join('\\t'.join(str(y) for y in x) for x in test_split))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}